# OpenBricks ğŸ§±

**The Open Source Data Lakehouse Platform**

OpenBricks is an open-source, self-hostable data analytics platform similar to Databricks, designed to run locally or on any cloud infrastructure. Like Supabase does for Firebase, OpenBricks brings the power of enterprise data platforms to everyone.

[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Docker](https://img.shields.io/badge/Docker-Ready-blue)](https://docker.com)

## ğŸ¯ Vision

OpenBricks aims to provide:
- **Data Lakehouse Architecture**: Combining the flexibility of data lakes with the reliability of data warehouses
- **Interactive Notebooks**: Multi-language support (Python, SQL, Scala, R) for data exploration
- **Scalable Query Engine**: Apache Spark-based distributed computing
- **Delta Lake Storage**: ACID transactions, time travel, and schema evolution
- **Self-Hosting**: Run locally with Docker or deploy to any cloud
- **No Vendor Lock-in**: Open source stack with portable data formats

## ğŸ—ï¸ Architecture

OpenBricks follows a modular, microservices architecture (similar to Supabase's approach):

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        OpenBricks Studio                        â”‚
â”‚                     (Web Dashboard / UI)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         API Gateway                             â”‚
â”‚              (REST & GraphQL APIs - Kong/Traefik)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Auth    â”‚  â”‚ Notebooks â”‚  â”‚   Query   â”‚  â”‚  Storage  â”‚
â”‚ Service   â”‚  â”‚  Service  â”‚  â”‚  Engine   â”‚  â”‚  Service  â”‚
â”‚ (JWT/SSO) â”‚  â”‚ (Jupyter) â”‚  â”‚  (Spark)  â”‚  â”‚(Delta/S3) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚              â”‚              â”‚              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     PostgreSQL (Metadata)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Object Storage (MinIO/S3)                    â”‚
â”‚                       Delta Lake Tables                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Core Services

| Service | Technology | Purpose |
|---------|------------|---------|
| **API Gateway** | Kong/Traefik | Unified API access, rate limiting, routing |
| **Auth Service** | Custom (Go) | JWT authentication, RBAC, SSO integration |
| **Notebooks** | JupyterHub | Interactive data science environment |
| **Query Engine** | Apache Spark | Distributed data processing |
| **Storage** | MinIO + Delta Lake | Object storage with ACID transactions |
| **Dashboard** | React/TypeScript | Web UI for platform management |
| **Metadata DB** | PostgreSQL | Catalogs, users, jobs metadata |

## ğŸš€ Quick Start

### Prerequisites

- Docker & Docker Compose (v2.0+)
- 8GB RAM minimum (16GB recommended)
- 20GB free disk space

### Local Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/danielgaio/openbricks.git
   cd openbricks
   ```

2. **Configure environment**
   ```bash
   cp .env.example .env
   # Edit .env with your settings
   ```

3. **Start OpenBricks**
   ```bash
   docker compose up -d
   ```

4. **Access the platform**
   - Dashboard: http://localhost:3000
   - Notebooks: http://localhost:8888
   - API: http://localhost:8080
   - Spark UI: http://localhost:4040

### Default Credentials

| Service | Username | Password |
|---------|----------|----------|
| Dashboard | admin | openbricks |
| JupyterHub | admin | openbricks |
| PostgreSQL | openbricks | openbricks |
| MinIO | openbricks | openbricks123 |

> âš ï¸ **Security Note**: Change all default passwords before production use!

## ğŸ“ Project Structure

```
openbricks/
â”œâ”€â”€ docker/                 # Docker configurations
â”‚   â”œâ”€â”€ api/               # API Gateway config
â”‚   â”œâ”€â”€ spark/             # Spark cluster config
â”‚   â””â”€â”€ nginx/             # Reverse proxy config
â”œâ”€â”€ services/              # Core services
â”‚   â”œâ”€â”€ api/               # REST/GraphQL API service
â”‚   â”œâ”€â”€ auth/              # Authentication service
â”‚   â”œâ”€â”€ notebooks/         # Jupyter notebook service
â”‚   â”œâ”€â”€ query-engine/      # Spark query engine
â”‚   â”œâ”€â”€ storage/           # Storage management service
â”‚   â””â”€â”€ dashboard/         # Web UI (React app)
â”œâ”€â”€ scripts/               # Utility scripts
â”‚   â”œâ”€â”€ setup.sh           # Initial setup
â”‚   â””â”€â”€ backup.sh          # Backup utilities
â”œâ”€â”€ docs/                  # Documentation
â”œâ”€â”€ docker-compose.yml     # Main compose file
â”œâ”€â”€ .env.example           # Environment template
â””â”€â”€ README.md
```

## ğŸ”§ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENBRICKS_PORT` | Main dashboard port | 3000 |
| `POSTGRES_PASSWORD` | Database password | openbricks |
| `MINIO_ROOT_PASSWORD` | Object storage password | openbricks123 |
| `JWT_SECRET` | JWT signing secret | (generate one!) |
| `SPARK_DRIVER_MEMORY` | Spark driver memory | 2g |
| `SPARK_EXECUTOR_MEMORY` | Spark executor memory | 2g |

### Scaling

For production deployments, adjust the following in `docker-compose.yml`:

```yaml
services:
  spark-worker:
    deploy:
      replicas: 3  # Number of Spark workers
      resources:
        limits:
          cpus: '4'
          memory: 8G
```

## ğŸŒ Cloud Deployment

### AWS
```bash
./scripts/deploy-aws.sh
```

### Google Cloud
```bash
./scripts/deploy-gcp.sh
```

### Azure
```bash
./scripts/deploy-azure.sh
```

### Kubernetes
```bash
kubectl apply -f k8s/
```

## ğŸ“Š Features

### Data Management
- [x] Delta Lake table support
- [x] Schema management and evolution
- [x] Data catalog with Unity-style governance
- [x] Time travel queries
- [ ] Data lineage tracking (coming soon)

### Compute
- [x] Apache Spark clusters
- [x] Auto-scaling workers
- [x] Job scheduling
- [ ] Serverless SQL (coming soon)
- [ ] ML Runtime (coming soon)

### Collaboration
- [x] Interactive notebooks
- [x] Real-time collaboration
- [x] Version control integration
- [ ] Comments and annotations (coming soon)

### Security
- [x] JWT authentication
- [x] Role-based access control (RBAC)
- [x] Row-level security
- [ ] SSO/SAML integration (coming soon)
- [ ] Audit logging (coming soon)

## ğŸ› ï¸ Development

### Setting up the development environment

```bash
# Install dependencies
npm install  # For dashboard
pip install -r requirements.txt  # For Python services

# Start services in dev mode
docker compose -f docker-compose.dev.yml up -d

# Run tests
npm test
pytest
```

### Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“š Documentation

- [Installation Guide](docs/installation.md)
- [Configuration Reference](docs/configuration.md)
- [API Documentation](docs/api.md)
- [Architecture Overview](docs/architecture.md)
- [Security Best Practices](docs/security.md)

## ğŸ”— Related Projects

OpenBricks is built on top of amazing open-source projects:

- [Apache Spark](https://spark.apache.org/) - Distributed computing engine
- [Delta Lake](https://delta.io/) - Open table format for data lakes
- [JupyterHub](https://jupyter.org/hub) - Multi-user notebook server
- [PostgreSQL](https://postgresql.org/) - Relational database
- [MinIO](https://min.io/) - S3-compatible object storage
- [Kong](https://konghq.com/) - API Gateway

## ğŸ“„ License

This project is licensed under the Apache License 2.0 - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

Inspired by:
- [Databricks](https://databricks.com/) - For pioneering the Lakehouse architecture
- [Supabase](https://supabase.com/) - For showing how to build open-source alternatives
- The open-source data community

---

<p align="center">
  <strong>Built with â¤ï¸ by the community, for the community</strong>
</p>